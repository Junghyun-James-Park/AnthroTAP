<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="AnthroTAP, an automated pipeline that leverages the SMPL model to generate pseudo-labeled data for human point tracking, enabling state-of-the-art performance with significantly less supervision and computation.">
  <meta name="keywords" content="AnthroTAP, Anthro-LD, Point tracking, Track Any Point (TAP)">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>AnthroTAP : Learning to Track Any Points from Human Motion</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://cvlab.kaist.ac.kr/">
        <span class="icon">
            <i class="fas fa-home"></i>
        </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://cvlab-kaist.github.io/locotrack/">
            LocoTrack
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">
            Learning to Track Any Points from Human Motion
          </h1>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://ines-hyeonsu-kim.github.io">Inès Hyeonsu Kim</a><sup>1*</sup>,
            </span>
            <span class="author-block">
              <a href="https://seokju-cho.github.io">Seokju Cho</a><sup>1*</sup>,
            </span>
            <span class="author-block">
              <a href="https://gabriel-huang.github.io">Jiahui Huang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://github.com/koojahyeok">Jahyeok Koo</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://junghyun-james-park.github.io/">Junghyun Park</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://joonyoung-cv.github.io">Joon-Young Lee</a><sup>2†</sup>,
            </span>
            <span class="author-block">
              <a href="https://cvlab.kaist.ac.kr/">Seungryong Kim</a><sup>1†</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>KAIST AI,</span>
            <span class="author-block"><sup>2</sup>Adobe Research,</span>
            <span class="author-block"><sup>3</sup>Seoul National University,</span>
            <span class="author-block"><sup>4</sup>Yonsei University</span>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>*</sup>Equal contribution</span>
            <span class="author-block"><sup>†</sup>Corresponding Authors</span>
          </div>

          <div class="is-size-6 publication-authors" style="margin-top: 10px">
            <span class="author-block"><b>ArXiv 2025</b></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper (Comming Soon)</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv (Comming Soon)</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Comming Soon)</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-database"></i>
                  </span>
                  <span>Data (Comming Soon)</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div id="results-carousel" class="carousel results-carousel" style="margin-top: -25px">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/267_241.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/563_300.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/890_300.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/567_300.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/710_300.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/368_300.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/751_251.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/880_300.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <h2 class="subtitle has-text-centered" style="margin-top: 10px">
        <b>AnthroTAP</b> generates highly complex pseudo-labeled data for point tracking <br>
        by leveraging the inherent complexities of human motion captured in videos.
      </h2>
    </div>
  </div>
</section>


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="margin-top: -20px">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Human motion, with its inherent complexities, such as non-rigid deformations,
            articulated movements, clothing distortions, and frequent occlusions caused by
            limbs or other individuals, provides a rich and challenging source of supervision that
            is crucial for training robust and generalizable point trackers. Despite the suitability
            of human motion, acquiring extensive training data for point tracking remains
            difficult due to laborious manual annotation. 
          </p>

          <p>
            Our proposed pipeline, <b>AnthroTAP</b>,
            addresses this by proposing an automated pipeline to generate pseudo-labeled
            training data, leveraging the Skinned Multi-Person Linear (SMPL) model. We
            first fit the SMPL model to detected humans in video frames, project the resulting
            3D mesh vertices onto 2D image planes to generate pseudo-trajectories, handle
            occlusions using ray-casting, and filter out unreliable tracks based on optical flow
            consistency. 
          </p>

          <p>
            A point tracking model trained on AnthroTAP annotated dataset
            achieves state-of-the-art performance on the TAP-Vid benchmark, surpassing other
            models trained on real videos while using \( 10,000\times \) less data and only 1 day in 4
            GPUs, compared to 256 GPUs used in recent state-of-the-art.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Overall Pipeline. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" style="margin-top: -10px">Overall Pipeline</h2>

        <img src="static/images/overall-pipeline.png" class="center">

        <div class="content has-text-justified" style="margin-top: 15px; text-align: left;">
          <b>AnthroTAP</b> extract human meshes using an off-the-shelf human mesh recovery model, and track points by projecting the mesh vertices.
          We also determine point visibility using ray-casting.
          In parallel, we extract optical flow and retain only reliable flow using forwardbackward consistency.
          Finally, we filter the trajectories by checking the consistency between the optical flow and the trajectories generated from the human mesh.
        </div>

      </div>
    </div>
    <!-- Overall Pipeline. -->
  </div>
</section>


<section class="section hero" style="padding-top: 0;">
  <div class="container is-max-desktop">
    <!-- Qualitative Comparisons. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" style="margin-top: 30px">Qualitative Comparisons</h2>
        <div class="content has-text-justified" style="margin-top: 15px; text-align: left;">
          Compared to previous state-of-the-art methods, <b>Anthro-LocoTrack</b>, LocoTrack-base model
          trained with the dataset generated by our pipeline, consistently demonstrates strong performance
          on highly deformable objects and severe occlusions. 
        </div>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr style="padding:0px">
              <td style="padding-right:1%;width:32%;vertical-align:middle">
                <p style="text-align: center;  margin-top: 5px; margin-bottom: 15px; font-size: 20px">CoTracker3</p>
              </td>
              <td style="padding-right:1%;width:32%;vertical-align:middle">
                <p style="text-align: center;  margin-top: 5px; margin-bottom: 15px; font-size: 20px">BootsTAPIR</p>
              </td>
              <td style="padding-right:1%;width:32%;vertical-align:middle">
                <p style="text-align: center;  margin-top: 5px; margin-bottom: 15px; font-size: 20px"><b>Anthro-LocoTrack (Ours)</b> </p>
              </td>
            </tr>
          </tbody>
        </table>
        <table style="margin-top: -10px">
          <tbody>
            <tr style="padding:0px">
              <td style="padding-right:1%;width:32%;vertical-align:middle"><div class="vsc-controller vcs-show"></div>
                <video onloadstart="this.playbackRate = 0.8;" controls="" autoplay muted loop playsinline height="100%">
                  <source src="./static/videos/cotrack_dog-agility_0.mp4" type="video/mp4">Sorry, your browser doesn't support embedded
                  videos.
                </video>
              </td>
              <td style="padding-right:1%;width:32%;vertical-align:middle"><div class="vsc-controller vcs-show"></div>
                <video onloadstart="this.playbackRate = 0.8;" controls="" autoplay muted loop playsinline height="100%">
                  <source src="./static/videos/boots_dog-agility_0.mp4" type="video/mp4">Sorry, your browser doesn't support embedded
                  videos.
                </video>
              </td>
              <td style="padding-right:1%;width:32%;vertical-align:middle"><div class="vsc-controller vcs-show"></div>
                <video onloadstart="this.playbackRate = 0.8;" controls="" autoplay muted loop playsinline height="100%">
                  <source src="./static/videos/ours_dog-agility_0.mp4" type="video/mp4">Sorry, your browser doesn't support embedded
                  videos.
                </video>
              </td>
            </tr><tr style="padding:0px">
              <td style="padding-right:1%;width:32%;vertical-align:middle"><div class="vsc-controller vcs-show"></div>
                <video onloadstart="this.playbackRate = 0.8;" controls="" autoplay muted loop playsinline height="100%">
                  <source src="./static/videos/cotrack_dogs-scale_0.mp4" type="video/mp4">Sorry, your browser doesn't support embedded
                  videos.
                </video>
              </td>
              <td style="padding-right:1%;width:32%;vertical-align:middle"><div class="vsc-controller vcs-show"></div>
                <video onloadstart="this.playbackRate = 0.8;" controls="" autoplay muted loop playsinline height="100%">
                  <source src="./static/videos/boots_dogs-scale_0.mp4" type="video/mp4">Sorry, your browser doesn't support embedded
                  videos.
                </video>
              </td>
              <td style="padding-right:1%;width:32%;vertical-align:middle"><div class="vsc-controller vcs-show"></div>
                <video onloadstart="this.playbackRate = 0.8;" controls="" autoplay muted loop playsinline height="100%">
                  <source src="./static/videos/ours_dogs-scale_0.mp4" type="video/mp4">Sorry, your browser doesn't support embedded
                  videos.
                </video>
              </td>
            </tr>
            <tr style="padding:0px">
              <td style="padding-right:1%;width:32%;vertical-align:middle"><div class="vsc-controller vcs-show"></div>
                <video onloadstart="this.playbackRate = 0.8;" controls="" autoplay muted loop playsinline height="100%">
                  <source src="./static/videos/cotrack_varanus-cage_0.mp4" type="video/mp4">Sorry, your browser doesn't support embedded
                  videos.
                </video>
              </td>
              <td style="padding-right:1%;width:32%;vertical-align:middle"><div class="vsc-controller vcs-show"></div>
                <video onloadstart="this.playbackRate = 0.8;" controls="" autoplay muted loop playsinline height="100%">
                  <source src="./static/videos/boots_varanus-cage_0.mp4" type="video/mp4">Sorry, your browser doesn't support embedded
                  videos.
                </video>
              </td>
              <td style="padding-right:1%;width:32%;vertical-align:middle"><div class="vsc-controller vcs-show"></div>
                <video onloadstart="this.playbackRate = 0.8;" controls="" autoplay muted loop playsinline height="100%">
                  <source src="./static/videos/ours_varanus-cage_0.mp4" type="video/mp4">Sorry, your browser doesn't support embedded
                  videos.
                </video>
              </td>
            </tr>
            <tr style="padding:0px">
              <td style="padding-right:1%;width:32%;vertical-align:middle"><div class="vsc-controller vcs-show"></div>
                <video onloadstart="this.playbackRate = 0.8;" controls="" autoplay muted loop playsinline height="100%">
                  <source src="./static/videos/cotrack_crossing_0.mp4" type="video/mp4">Sorry, your browser doesn't support embedded
                  videos.
                </video>
              </td>
              <td style="padding-right:1%;width:32%;vertical-align:middle"><div class="vsc-controller vcs-show"></div>
                <video onloadstart="this.playbackRate = 0.8;" controls="" autoplay muted loop playsinline height="100%">
                  <source src="./static/videos/boots_crossing_0.mp4" type="video/mp4">Sorry, your browser doesn't support embedded
                  videos.
                </video>
              </td>
              <td style="padding-right:1%;width:32%;vertical-align:middle"><div class="vsc-controller vcs-show"></div>
                <video onloadstart="this.playbackRate = 0.8;" controls="" autoplay muted loop playsinline height="100%">
                  <source src="./static/videos/ours_crossing_0.mp4" type="video/mp4">Sorry, your browser doesn't support embedded
                  videos.
                </video>
              </td>
            </tr>
          </tbody>
        </table>
      </div>
    </div>
    <!-- Qualitative Comparisons. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Quantitative Comparison. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" style="margin-top: -20px">Quantitative Comparison</h2>

        <img src="static/images/main-quan.png" class="center">

        <div class="content has-text-justified" style="margin-top: 15px; text-align: left;">
          LocoTrack-B, trained with our approach, shows a significant performance improvement across all metrics
          and datasets on <b>TAP-Vid benchmark</b> and <b>RoboTAP</b>, even with \( 11\times \) smaller training dataset
          than CoTracker3 in terms of the number of videos, and \( 1,000\times \) smaller dataset used in BootsTAPIR
          in terms of the number of training frames.
          
          Furthermore, in terms of position accuracy (\(< δ_{avg}^x \)), our model achieves the best performance on every dataset.
        </div>

      </div>
    </div>
    <!-- Quantitative Comparison. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Concurrent Work. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            <a href="https://co-tracker.github.io//">CoTracker</a> is the first iteration of this work. In the follow-up, we significantly simplified the architecture and showed that it is possible to improve point trackers
              by trainining them on real data with pseudo-labels produced by other synthetic-trained models. Check out CoTracker as well!
          </p>
          <p>
            <a href="https://bootstap.github.io/">BootsTAPIR</a> is a bootstrapped version of <a href="https://deepmind-tapir.github.io/">TAPIR</a>,  
            a feed-forward point tracker with a matching stage inspired by <a href="https://arxiv.org/abs/2211.03726">TAP-Vid</a> and a refinement stage inspired by <a href="https://particle-video-revisited.github.io/">PIPs</a>. The model
            demonstrates accurate tracking of visible points, which improves after scaling. However, it struggles to predict the positions of occluded points.
          </p>
          <p>
            <a href="https://cvlab-kaist.github.io/locotrack/">LocoTrack</a> is an efficient point-tracking model that accuractely tracks visible points. It introduces 4D correlations that we also use in the architecture of CoTracker3.
          </p>
          <p>
            <a href="https://shape-of-motion.github.io/">Shape of Motion</a> is a test-time optimisation method for dynamic reconstrucion that proposes a new motion representation. It produces accurate 3D tracks and shows visuals with nice dynamic reconstructions.
          </p>
          <p>
            <a href="https://vggsfm.github.io/">VGGSfM</a> is the first end-to-end differentiable structure-from-motion pipeline that outperforms traditional algorithms. Point tracking is one of its components. 
          </p>
          
          

          
        </div>
      </div>
    </div>
    <!-- Concurrent Work. -->
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{kim2025learning,
  author    = {Kim, In{\`e}s Hyeonsu and Cho, Seokju and Huang, Jiahui and Koo, Jahyeok and Park, Junghyun and Lee, Joon-Young and Kim, Seungryong},
  title     = {Learning to Track Any Points from Human Motion},
  journal   = {arXiv preprint},
  year      = {2025},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website adapted from the following <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>