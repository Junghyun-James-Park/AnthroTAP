<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="AnthroTAP, an automated pipeline that leverages the SMPL model to generate pseudo-labeled data for human point tracking, enabling state-of-the-art performance with significantly less supervision and computation.">
  <meta name="keywords" content="AnthroTAP, Anthro-LD, Point tracking, Track Any Point (TAP)">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Learning to Track Any Points from Human Motion</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://cvlab.kaist.ac.kr/">
        <span class="icon">
            <i class="fas fa-home"></i>
        </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://cvlab-kaist.github.io/locotrack/">
            LocoTrack
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">
            Learning to Track Any Points from Human Motion
          </h1>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://ines-hyeonsu-kim.github.io">Inès Hyeonsu Kim</a><sup>1*</sup>,
            </span>
            <span class="author-block">
              <a href="https://seokju-cho.github.io">Seokju Cho</a><sup>1*</sup>,
            </span>
            <span class="author-block">
              <a href="https://gabriel-huang.github.io">Jiahui Huang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://github.com/koojahyeok">Jahyeok Koo</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://junghyun-james-park.github.io/">Junghyun Park</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://joonyoung-cv.github.io">Joon-Young Lee</a><sup>2†</sup>,
            </span>
            <span class="author-block">
              <a href="https://cvlab.kaist.ac.kr/">Seungryong Kim</a><sup>1†</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>KAIST AI,</span>
            <span class="author-block"><sup>2</sup>Adobe Research,</span>
            <span class="author-block"><sup>3</sup>Seoul National University,</span>
            <span class="author-block"><sup>4</sup>Yonsei University</span>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>*</sup>Equal contribution</span>
            <span class="author-block"><sup>†</sup>Corresponding Authors</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper (Comming Soon)</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv (Comming Soon)</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Comming Soon)</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-database"></i>
                  </span>
                  <span>Data (Comming Soon)</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <h2 class="subtitle has-text-centered">
        <b>AnthroTAP</b> generates highly complex pseudo-labeled data for point tracking <br>
        by leveraging the inherent complexities of human motion captured in videos.
      </h2>
    </div>
  </div>
</section>


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="margin-top: -20px">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Human motion, with its inherent complexities, such as non-rigid deformations,
            articulated movements, clothing distortions, and frequent occlusions caused by
            limbs or other individuals, provides a rich and challenging source of supervision that
            is crucial for training robust and generalizable point trackers. Despite the suitability
            of human motion, acquiring extensive training data for point tracking remains
            difficult due to laborious manual annotation. 
          </p>

          <p>
            Our proposed pipeline, <b>AnthroTAP</b>,
            addresses this by proposing an automated pipeline to generate pseudo-labeled
            training data, leveraging the Skinned Multi-Person Linear (SMPL) model. We
            first fit the SMPL model to detected humans in video frames, project the resulting
            3D mesh vertices onto 2D image planes to generate pseudo-trajectories, handle
            occlusions using ray-casting, and filter out unreliable tracks based on optical flow
            consistency. 
          </p>

          <p>
            A point tracking model trained on AnthroTAP annotated dataset
            achieves state-of-the-art performance on the TAP-Vid benchmark, surpassing other
            models trained on real videos while using \( 10,000\times \) less data and only 1 day in 4
            GPUs, compared to 256 GPUs used in recent state-of-the-art.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Overall Pipeline. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" style="margin-top: -20px">Overall Pipeline</h2>

        <img src="static/images/overall-pipeline.png" class="center">

        <div class="content has-text-justified" style="margin-top: 15px; text-align: left;">
          <b>AnthroTAP</b> extract human meshes using an off-the-shelf human mesh recovery model, and track points by projecting the mesh vertices.
          We also determine point visibility using ray-casting.
          In parallel, we extract optical flow and retain only reliable flow using forwardbackward consistency.
          Finally, we filter the trajectories by checking the consistency between the optical flow and the trajectories generated from the human mesh.
        </div>

      </div>
    </div>
    <!-- Overall Pipeline. -->
  </div>
</section>


<section class="section hero" style="padding-top: 0;">
  <div class="container is-max-desktop">
    <!-- Qualitative Results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Qualitative Results</h2>
        <!--<h2 class="title is-3 has-text-centered"><br>Qualitative Results</h2>
        <div class="content has-text-justified" style="margin-top: 15px; text-align: left; font-size: 20px;">
          <strong>Qualitative comparison of complex real-world video tracking.</strong> We qualitatively compare the results generated by Chrono
          with those from other commonly used backbones in point tracking. Our model demonstrates better smoothness and precision than other competitors.
        </div>-->
        <div class="content has-text-justified" style="margin-top: 15px; text-align: left;">
          Compared to previous state-of-the-art methods, <b>Anthro-LocoTrack</b>, LocoTrack-base model
          trained with the dataset generated by our pipeline, consistently demonstrates strong performance
          on highly deformable objects and severe occlusions. 
        </div>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr style="padding:0px">
              <td style="padding-right:1%;width:32%;vertical-align:middle">
                <p style="text-align: center;  margin-top: 5px; margin-bottom: 15px; font-size: 20px">CoTracker3</p>
              </td>
              <td style="padding-right:1%;width:32%;vertical-align:middle">
                <p style="text-align: center;  margin-top: 5px; margin-bottom: 15px; font-size: 20px">BootsTAPIR</p>
              </td>
              <td style="padding-right:1%;width:32%;vertical-align:middle">
                <p style="text-align: center;  margin-top: 5px; margin-bottom: 15px; font-size: 20px"><b>Anthro-LocoTrack (Ours)</b> </p>
              </td>
            </tr>
          </tbody>
        </table>
        <table>
          <tbody>
            <tr style="padding:0px">
              <td style="padding-right:1%;width:32%;vertical-align:middle"><div class="vsc-controller vcs-show"></div>
                <video onloadstart="this.playbackRate = 0.8;" controls="" autoplay muted loop playsinline height="100%">
                  <source src="./static/videos/cotrack_dog-agility_0.mp4" type="video/mp4">Sorry, your browser doesn't support embedded
                  videos.
                </video>
              </td>
              <td style="padding-right:1%;width:32%;vertical-align:middle"><div class="vsc-controller vcs-show"></div>
                <video onloadstart="this.playbackRate = 0.8;" controls="" autoplay muted loop playsinline height="100%">
                  <source src="./static/videos/boots_dog-agility_0.mp4" type="video/mp4">Sorry, your browser doesn't support embedded
                  videos.
                </video>
              </td>
              <td style="padding-right:1%;width:32%;vertical-align:middle"><div class="vsc-controller vcs-show"></div>
                <video onloadstart="this.playbackRate = 0.8;" controls="" autoplay muted loop playsinline height="100%">
                  <source src="./static/videos/ours_dog-agility_0.mp4" type="video/mp4">Sorry, your browser doesn't support embedded
                  videos.
                </video>
              </td>
            </tr><tr style="padding:0px">
              <td style="padding-right:1%;width:32%;vertical-align:middle"><div class="vsc-controller vcs-show"></div>
                <video onloadstart="this.playbackRate = 0.8;" controls="" autoplay muted loop playsinline height="100%">
                  <source src="./static/videos/cotrack_dogs-scale_0.mp4" type="video/mp4">Sorry, your browser doesn't support embedded
                  videos.
                </video>
              </td>
              <td style="padding-right:1%;width:32%;vertical-align:middle"><div class="vsc-controller vcs-show"></div>
                <video onloadstart="this.playbackRate = 0.8;" controls="" autoplay muted loop playsinline height="100%">
                  <source src="./static/videos/boots_dogs-scale_0.mp4" type="video/mp4">Sorry, your browser doesn't support embedded
                  videos.
                </video>
              </td>
              <td style="padding-right:1%;width:32%;vertical-align:middle"><div class="vsc-controller vcs-show"></div>
                <video onloadstart="this.playbackRate = 0.8;" controls="" autoplay muted loop playsinline height="100%">
                  <source src="./static/videos/ours_dogs-scale_0.mp4" type="video/mp4">Sorry, your browser doesn't support embedded
                  videos.
                </video>
              </td>
            </tr>
            <tr style="padding:0px">
              <td style="padding-right:1%;width:32%;vertical-align:middle"><div class="vsc-controller vcs-show"></div>
                <video onloadstart="this.playbackRate = 0.8;" controls="" autoplay muted loop playsinline height="100%">
                  <source src="./static/videos/cotrack_varanus-cage_0.mp4" type="video/mp4">Sorry, your browser doesn't support embedded
                  videos.
                </video>
              </td>
              <td style="padding-right:1%;width:32%;vertical-align:middle"><div class="vsc-controller vcs-show"></div>
                <video onloadstart="this.playbackRate = 0.8;" controls="" autoplay muted loop playsinline height="100%">
                  <source src="./static/videos/boots_varanus-cage_0.mp4" type="video/mp4">Sorry, your browser doesn't support embedded
                  videos.
                </video>
              </td>
              <td style="padding-right:1%;width:32%;vertical-align:middle"><div class="vsc-controller vcs-show"></div>
                <video onloadstart="this.playbackRate = 0.8;" controls="" autoplay muted loop playsinline height="100%">
                  <source src="./static/videos/ours_varanus-cage_0.mp4" type="video/mp4">Sorry, your browser doesn't support embedded
                  videos.
                </video>
              </td>
            </tr>
            <tr style="padding:0px">
              <td style="padding-right:1%;width:32%;vertical-align:middle"><div class="vsc-controller vcs-show"></div>
                <video onloadstart="this.playbackRate = 0.8;" controls="" autoplay muted loop playsinline height="100%">
                  <source src="./static/videos/cotrack_crossing_0.mp4" type="video/mp4">Sorry, your browser doesn't support embedded
                  videos.
                </video>
              </td>
              <td style="padding-right:1%;width:32%;vertical-align:middle"><div class="vsc-controller vcs-show"></div>
                <video onloadstart="this.playbackRate = 0.8;" controls="" autoplay muted loop playsinline height="100%">
                  <source src="./static/videos/boots_crossing_0.mp4" type="video/mp4">Sorry, your browser doesn't support embedded
                  videos.
                </video>
              </td>
              <td style="padding-right:1%;width:32%;vertical-align:middle"><div class="vsc-controller vcs-show"></div>
                <video onloadstart="this.playbackRate = 0.8;" controls="" autoplay muted loop playsinline height="100%">
                  <source src="./static/videos/ours_crossing_0.mp4" type="video/mp4">Sorry, your browser doesn't support embedded
                  videos.
                </video>
              </td>
            </tr>
          </tbody>
        </table>
      </div>
    </div>
    <!-- Qualitative Results. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Visual Effects</h2>
          <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/dollyzoom-stacked.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
        <h2 class="title is-3">Matting</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p>
            <video id="matting-video" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>
    <!--/ Matting. -->

    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Animation</h2>

        <!-- Interpolating. -->
        <h3 class="title is-4">Interpolating states</h3>
        <div class="content has-text-justified">
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/>
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <h3 class="title is-4">Re-rendering the input video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div>
        <!--/ Re-rendering. -->

      </div>
    </div>
    <!--/ Animation. -->


    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{kim2025learning,
  author    = {Kim, In{\`e}s Hyeonsu and Cho, Seokju and Huang, Jiahui and Koo, Jahyeok and Park, Junghyun and Lee, Joon-Young and Kim, Seungryong},
  title     = {Learning to Track Any Points from Human Motion},
  journal   = {arXiv preprint},
  year      = {2025},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website adapted from the following <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>